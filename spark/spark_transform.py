from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StringType
import requests

# Kh·ªüi t·∫°o Spark session
spark = SparkSession.builder.appName("KafkaSentimentTransform").getOrCreate()

# Schema cho d·ªØ li·ªáu Kafka
schema = StructType().add("content", StringType()).add("user", StringType())

# ƒê·ªçc d·ªØ li·ªáu t·ª´ Kafka
df = (
    spark.readStream
    .format("kafka")
    .option("kafka.bootstrap.servers", "kafka:29092")
    .option("subscribe", "load_raw_data")
    .option("startingOffsets", "latest")
    .load()
)

# Parse JSON
parsed_df = (
    df.selectExpr("CAST(value AS STRING)")
    .select(from_json(col("value"), schema).alias("data"))
    .select("data.*")
)

# H√†m x·ª≠ l√Ω theo batch
def process_batch(batch_df, epoch_id):
    if batch_df.rdd.isEmpty():
        print(f"[Epoch {epoch_id}] ‚ùóBatch is empty.")
        return

    # Convert sang pandas ƒë·ªÉ x·ª≠ l√Ω d·ªÖ h∆°n
    pdf = batch_df.toPandas()
    texts = pdf["content"].tolist()

    print(f"[Epoch {epoch_id}] üöÄ Processing {len(texts)} texts...")

    # G·ªçi API batch
    try:
        response = requests.post("http://model-sentiment-service:5000/predict_batch", json={"texts": texts})
        sentiments = response.json().get("labels", ["error"] * len(texts))
    except Exception as e:
        print(f"[Epoch {epoch_id}] ‚ùå Error calling API: {e}")
        sentiments = ["error"] * len(texts)

    # In log k·∫øt qu·∫£ t·ª´ng d√≤ng
    for i, (text, sentiment) in enumerate(zip(texts, sentiments)):
        print(f"[Epoch {epoch_id}] #{i+1}: \"{text}\" ‚Üí {sentiment}")

    # G√°n l·∫°i k·∫øt qu·∫£ v√†o pandas v√† convert ng∆∞·ª£c l·∫°i
    pdf["sentiment"] = sentiments
    result_sdf = spark.createDataFrame(pdf)

    # Ghi v√†o Kafka
    (
        result_sdf
        .selectExpr("to_json(struct(*)) AS value")
        .write
        .format("kafka")
        .option("kafka.bootstrap.servers", "kafka:29092")
        .option("topic", "social_sentiment_cleaned")
        .save()
    )

    # Ghi v√†o Parquet
    (
        result_sdf
        .write
        .mode("append")
        .parquet("/data/social_sentiment_cleaned")
    )

    print(f"[Epoch {epoch_id}] ‚úÖ Done processing batch.\n")

# D√πng foreachBatch ƒë·ªÉ stream
(
    parsed_df.writeStream
    .foreachBatch(process_batch)
    .option("checkpointLocation", "/tmp/spark-checkpoint-foreach")
    .start()
    .awaitTermination()
)
# ƒê√≥ng Spark session
# spark.stop()